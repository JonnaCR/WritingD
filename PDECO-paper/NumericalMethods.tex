\subsection{Pseudospectral method for the forward problem}\label{sec:Method_PseudospectralForward}

As described in Section~\ref{sec:Background_Pseudospectral}, we solve the forward problem using
Chebyshev pseudospectral methods, in particular implemented in Matlab using 2DChebClass~\cite{NGYSK17,2DChebClass}
%
The principal novelties of the method concern the computation of convolution integrals and the implementation
of spatial boundary conditions; the boundary conditions in time will be discussed in the following section.

As described in~\cite{NGYSK17}, the convolution integrals are computed in real space, in contrast to many implementations
in which they are computed via Fourier transforms.  In this latter case, the domain must be padded to ensure that
all functions are (approximately) periodic, with suitably large padding regions to ensure that the edges of the 
physical domain do not interact with each other.  The principal advantage of this method is that it is
computationally cheap, with requiring only fast Fourier transforms and multiplication of functions.  The 
disadvantages are that, as above, one needs to pad the domain (which effectively wastes computational power
in computing a lot of zeros), and the boundary conditions can be challenging to impose on such an extended
grid.  The spatial method can be implemented by a single matrix--vector multiplication, with the matrix
precomputed for all time steps and is implemented on the physical domain, allowing efficient implementation
of the boundary conditions.  In short, for a convolution of functions $f$ and $\rho$,
\[
	I_f (\rho)= \int f(\vec{x}-\vec{y}) \rho(\vec{y}) ~ {\rm d}\vec{y}
\]
we compute a matrix $M_f$, such that for the discretized vector $\boldsymbol{\rho}$, we may approximate the
convolution integral via the matrix multiplication $M_f \boldsymbol{\rho} \approx I_f(\rho)$.  Note that
the matrix $M_f$ is independent of $\rho$, and as such may be precomputed.  In fact, it may be computed in parallel
for each computational grid point.

As is standard, after discretization, in this case through the use of (mapped) Chebyshev pseudospectral points,
the forward PDE(s) are converted into a system of ODEs.  For example, the diffusion equation becomes
\begin{equation}
	\frac{{\rm d}}{{\rm d} t} \boldsymbol{\rho} = D_2 \boldsymbol{\rho}, \qquad \mbox{+ IC and BC},
	\label{eq:DiscretizedDiffusion}
\end{equation}
where $\boldsymbol{\rho}$ is a vector of values of the solution at each of the Chebyshev points, and $D_2$ is the
Chebyshev second order differentiation matrix.
In the interior of the domain, this can be solved using essentially
any ODE solver; the challenge lies in imposing the correct spatial boundary conditions.  One standard approach
is to modify the matrix on the right hand side of \eqref{eq:DiscretizedDiffusion} so that the boundary conditions
are automatically satisfied.  This is known as `boundary-boardering'~\cite{B01}.  For simple boundary conditions, such as
homogenenous Dirichlet or Neumann, such an approach is relatively straightforward.  For example, for homogeneous Dirichlet,
assuming that the initial conditions satisfy the boundary conditions, it is sufficient to set the first and last rows
and columns of $D2$ to zero.  For homogeneous Neumann, there is a similar approach (see~\cite{T00}), which becomes
more involved with more complex right hand sides of the PDE.  Another approach is to restrict the computation
to interpolants (solutions) which satisfy the boundary conditions; we do not discuss this here as it is highly
non-trivial for the non-linear, non-local problems that we are interested in.

Here we take a more general approach.  The imposition of spatial boundary conditions can be seen as 
extending the discretized system of ODEs to a system of differential-algebraic equations, where the discretized
PDE is solved on the interior of the domain, and the boundary conditions correspond to algebraic equations.
There are various numerical methods to solve such differential-algebraic equations, e.g.~\cite{SEK99}.
The main advantage here is that the numerical method does not have to be explicitly adapted when 
one changes the boundary conditions; one simply has to specify different algebraic constraints that correspond
to the boundary conditions.  In fact, the 2DChebClass code automatically identifies the boundary of various
geometries, allowing a simple implementation of this approach.

\subsection{Pseudospectral method for the adjoint equation}\label{sec:Method_PseudospectralPDECO}

For the optimization problem, we have a pair of coupled PDEs, the forward PDE with an initial time condition and the adjoint equation with a final time condition. Hence, one cannot use a standard time-stepping scheme, due to the negative Laplacian having the same sign as the time derivative
in the state equation, and the opposite sign in the adjoint equation.(+++ Note that above it is written differently: both terms on LHS, therefore opposite true, may be confusing+++)
This must be accounted for to ensure stable solutions. One approach
is to apply a backward Euler method for the time derivative in the state equation, with the adjoint operator applied to the adjoint equation, whereupon a huge-scale coupled
system of equations is obtained from matrices arising at each time-step. These may be tackled using a preconditioned iterative method, following the
methodology in \cite{PS13,PSW12,SW10}, for instance.  As above,
as well as boundary conditions in time, there are also boundary conditions in space.  In order to utilise our efficient and accurate forward solver, we reverse time in the adjoint problem, resulting in a set of equations with initial conditions, so that the pseudospectral method can be applied, as described in Section \ref{sec:Method_PseudospectralForward}. However, the forward and adjoint equations are coupled non-locally in time; the adjoint equation requires the value of the state variable at later times, so the two equations cannot be solved simultaneously. This difficulty is addressed using the fixed point algorithm presented in the next section.  

\subsection{Optimization Solver}\label{sec:Method_Solver}
The remaining challenges are to (i) determine the time discretization for the optimality system;
(ii) choose a suitable optimization scheme. For (i), we again choose a Chebyshev pseudospectral scheme (1D in time), which, assuming that the solutions are smooth in time, leads to exponentially accurate interpolation. In [[REF]], we compare this approach to a standard finite difference scheme. As mentioned in Section \ref{sec:Method_PseudospectralForward}, the system of ODEs and the algebraic-differential equations, resulting from applying the pseudospectral method, can be solved with a standard ODE solver. In this paper, the Matlab inbuilt ODE solver \texttt{ode15s} is used.
For (ii), we note that the choice of optimization solver depends strongly on the nature of the solution, and the amount of information available.  For ease of implementation, and to remove the 
need to, e.g.\ analytically compute the Jacobian, we use a fixed point method. However, we note that this approach is highly modular and it is straightforward to replace this solver with any other optimization routine.
In order to solve an optimality system of the form derived in Section \ref{sec:Optimality}, we adapt a fixed point method, also known as sweeping algorithm in the literature, for example in~\cite{ACFK}, to solve the system of equations iteratively. 
\\

In the following we denote the discretized versions of the variables $\rho$, $\adj$ and $\vec{w}$ with $P$, $Q$ and $W$, respectively. Each of these matrices is of the form $A = [\boldsymbol{a_0}, \boldsymbol{a_1}, ... ,\boldsymbol{a_n}]$, where the vectors $\boldsymbol{a_k}$ represents the solution at the discretized time $k \in 0,1,....,n$, where $n$ is the number of time points. In particular, the first column of $P$, denoted by $\boldsymbol{\rho_0}$, corresponds to the initial condition $\rho(\vec{x},0)$. If the spatial domain is one-dimensional, $P$, $Q$ and $W$ are of size $N \times n$, where $N$ is the number of spatial points. In the two-dimensional case, $P$ and $Q$ are of size $(N_1N_2) \times n$, where $N_1$ is the number of spatial points in the direction of $x_1$ and $N_2$ the points along the $x_2$ axis. Generally $N_1 = N_2$. The discretized control $W$ is, for linear control problems, also $(N_1N_2) \times n$ dimensional, while it is $(2N_1N_2) \times n$ dimensional for nonlinear control problems. This is due to the fact that the control is represented by a vector field, when applied nonlinearly.
\\
\\
The optimization algorithm is initialized with a guess for the control, $W^{(0)}$. Then, during each iteration, denoted by $i$, the following steps are computed:
\vspace{0.1cm}
\begin{enumerate}
	\item Starting with a guess for the control $W^{(i)}$ as input variable, the corresponding state $P^{(i)}$ is found by solving the forward equation.
	\item In a next step, the adjoint $Q^{(i)}$ is found by computing the adjoint equation, using $W^{(i)}$ and $P^{(i)}$. Since $P^{(i)}$ contains the solution for all discretized times $k \in 0,1,...,n$, this circumvents issues resulting from the non-local coupling in time, mentioned in Section \ref{sec:Method_PseudospectralPDECO}. As illustrated in the same section, time is reversed in the adjoint equation, so that the result is a matrix $\tilde{Q}^{(i)} =  [\boldsymbol{\adj_n},\boldsymbol{\adj_{n-1}}, ..., \boldsymbol{\adj_1} ]$. The columns of $\tilde{Q}^{(i)}$ are permuted to obtain the solution  $Q^{(i)}$.
	\item The gradient equation is solved, given the solutions $P^{(i)}$ and $Q^{(i)}$. This results in a new value for the control, $W^{(i)}_g$.
	\item  The convergence of the optimization scheme is measured by computing the error between $W^{(i)}$ and $W^{(i)}_{g}$, as described in Section \ref{sec:Method_Validation}. 
	\begin{itemize}
		\item  If this error is lower than a set tolerance, the optimality system is self-consistent. This implies that the solution triplet ($\bar{P},\bar{W},\bar{Q}$) solves the (discretized) optimality system, and is therefore an optimal solution to the PDE-constrained optimization problem of interest.
		\item If the error is above the optimality tolerance, step 5 is executed.
	\end{itemize}
	\item Finally, the update $W^{(i+1)}$ is a linear combination of the current guess $W^{(i)}$, and the value obtained in 3., $W^{(i)}_{g}$, with mixing rate $\lambda \in [0,1]$:
	\begin{align*}
	W^{(i+1)} = (1-\lambda)W^{(i)} + \lambda W^{(i)}_{g}.
	\end{align*}
	The guess for the control is updated from $W^{(i)} $ to $W^{(i+1)} $ and steps 1.-5. are repeated until the method converges. 
\end{enumerate}
\vspace{0.3cm}
The update scheme in 5., with mixing rate $\lambda$, is known to stabilise fixed point methods, ++Ben to add references++. Typical values of $\lambda$, which provide stable convergence, lie between $0.1$ and $0.001$. Throughout this paper, $\lambda =0.01$, unless stated otherwise. This mixing scheme is equivalent to the updating scheme presented in~\cite{Burger1}. 
Note that, while the solutions $P^{(i)}$ and $Q^{(i)}$ change in each iteration, the initial condition $\boldsymbol{\rho_0}$ and final time condition $\boldsymbol{\adj_n}$ remain unchanged throughout the process. Therefore, the only variable inducing a change in the solution is $W^{(i)}$.




\subsection{Measures of Accuracy}\label{sec:Method_Validation}


All errors in Section \ref{sec:Method_Validation} and Section \ref{sec:Expts} are calculated between a variable of interest, $y$, and $y_R$, the reference value that $y$ is compared to. For example, when measuring convergence of the fixed point scheme, described in Section \ref{sec:Method_Solver}, $y = W^{(i)}_g$ and $y_R = W^{(i)}_i$. Alternatively, when investigating a known test problem, see Section \ref{sec:Method_Validation}, $y$ is a numerical solution and $y_R$ is an exact solution. The error measure $\mathcal{E}$ is composed of an $L^2$ error in space and an $L^\infty$ error in time. The relative $L^2$ error in the spatial direction is:
\begin{align*}
\mathcal{E}_{Rel}(t) = \frac{|| y(x,t) - y_{R}(x,t)||_{L^2(\Omega)} }{||y_R(x,t) + 10^{-10}||_{L^2(\Omega)}},
\end{align*}
where the small additional term on the denominator prevents division by zero.
Furthermore, the absolute $L^2$ error is:
\begin{align*}
\mathcal{E}_{Abs}(t) = || y(x,t) - y_R(x,t)||_{L^2(\Omega)}.
\end{align*}
Then, an $L^\infty$ error in time is taken of the minimum of $\mathcal{E}_{Rel}$ and $\mathcal{E}_{Abs}$, to obtain the error of interest:
\begin{align*}
\mathcal{E} = \max_{t \in [0,T]}\left[\min\left(\mathcal{E}_{Rel}(t), \mathcal{E}_{Abs}(t)\right)\right].
\end{align*}
The minimum between absolute and relative spatial error is taken to avoid choosing an erogenously large relative error, caused by division of one small term by another.

As a benchmark, we compared the fixed point scheme to Matlab's inbuilt \texttt{fsolve} function. It uses the trust-region-dogleg algorithm, see~\cite{Powell1}, to solve the optimality system of interest. While it is very robust, it is also much slower than the fixed point method, which works reliably for the types of problems considered in this paper. A comparison is given in Appendix \ref{app:fsolveComparison}. Numerical results for specific test problems with exact solutions are supplied in Appendix \ref{app:TestProblems}. Further tests to validate the method are presented in Appendix \ref{app:TestProblemsPerturbed}.
