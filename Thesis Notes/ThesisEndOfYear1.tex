\documentclass[11pt, a4paper]{article}
%\usepackage{proj1}
\usepackage{natbib}
\usepackage{fancyhdr}  
\usepackage{subcaption}
\usepackage{caption}
\usepackage{graphicx}
\linespread{1.25} 
\setlength{\parindent}{0cm}
\graphicspath{{Images/}}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{bbm}

%\usepackage[sc,osf]{mathpazo}
\usepackage{subcaption}
\usepackage[a4paper, top=1in, left=1.0in, right=1.0in, bottom=1in, includehead, includefoot]{geometry} %Usually have top as 1in

\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}


\hypersetup{colorlinks,linkcolor={black},citecolor={blue},urlcolor={black}}
\usepackage{color}
\urlstyle{same}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

%\newcommand{\Sta}{\rho}
\newcommand{\Adj}{p}
\newcommand{\adj}{q}
%\newcommand{\Con}{u}
\newcommand{\Sta}{\rho}
\newcommand{\Stav}{\mathbf{v}}
\newcommand{\Adja}{\mathbf{p}_\Sigma}
\newcommand{\Adjb}{q}
\newcommand{\Adjc}{p_{\partial \Sigma}}
\newcommand{\Con}{\mathbf{f}}
\newcommand{\nor}{\mathbf{n}}



\title{End Of Year Report/ Part of Thesis Draft}
\author{Jonna C. Roden\\ \\Supervision by Dr Ben Goddard and Dr John Pearson\\ \\ \vspace{0.5cm} MIGSAA}
\date{\today}


\pagenumbering{gobble}
\begin{document}
	\lstset{language=Matlab,%
		%basicstyle=\color{red},
		breaklines=true,%
		morekeywords={matlab2tikz},
		keywordstyle=\color{blue},%
		morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
		identifierstyle=\color{black},%
		stringstyle=\color{mylilas},
		commentstyle=\color{mygreen},%
		showstringspaces=false,%without this there will be a symbol in the places where there is a space
		numbers=left,%
		numberstyle={\tiny \color{black}},% size of the numbers
		numbersep=9pt, % this defines how far the numbers are from the text
		emph=[1]{for,end,break},emphstyle=[1]\color{blue}, %some words to emphasise
		%emph=[2]{word1,word2}, emphstyle=[2]{style},    
		basicstyle=\footnotesize\ttfamily,
	}
	
	\maketitle
	\begin{abstract}
		Sum up the below here!
		
	\end{abstract}
	
	\newpage
	\section*{Acknowledgements}
	+Acknowledge People Here+
	\newpage
	\pagenumbering{Roman} 
	\tableofcontents
	%\newpage
	%\listoffigures
	%\listoftables
	\newpage
	\pagenumbering{arabic} % Switch to normal numbers
	%\pagestyle{fancy}
















\section{Introduction}
++ Archer paper has examples for applications - also check extended project and other papers for this ++\\
+++get quotes right, figure out how ;) ++

\section{Background on DDFT Stuff}
+ Archer, Marconi and Tarazona  +\\
+ Archer ref 12, 13 for DFT free energy (below eqn 5) and other refs around there +
+ Archer ref 24 for kramers equation. is this the link with eqn 1? +
\section{Background on PDE-Constrained Optimization}
\section{Literature review on PDE-Constrained Optimization for these PDEs}
\input{MeanFieldOptimalControl.tex}
\section{Deriving optimality conditions}

\subsection{Standard Problems}
- derive optimality conditions for all the systems we need \\
- Flow/Force\\
- Neumann / Dirichlet/ Dirichlet non-zero/ Mixed\\
\subsection{Archer Optimality Conditions}
- Archer paper optimality conditions (good opportunity to link to some background stuff)
\input{OptimalityConditionsArcher.tex}
\subsection{Subdomain Observation and Non-Constant Flux}
- boundary/ subdomain control/observation, non-constant flux stuff\\
\input{NonconstantFluxSubdomainObservation.tex}
\subsection{Boundary Observation and Non-Constant Flux}
+++ probably combine with above section +++
%\input{NonconstantFluxBoundaryObservation.tex}
\section{Numerical Methods}
\subsection{Pseudospectral Methods}
- pseudospectral methods (why they are better than others in what we do, what else would be useful, what type of method exactly, how does it work)
\subsection{Multiple Shooting Method}
- multiple shooting background (how is it done in literature, how do we do it)
\subsection{Fixed Point Algorithm}
- Kalise and Burger papers, sweeping algorithms (and our adaptation - or adaptation in next section)\\
- below copy and paste from paper draft. misses intro because ben wrote it -- need to write my own! ++

In the following, we denote the discretized versions of the variables $\rho$, $\adj$ and $\vec{w}$ with $P$, $Q$ and $W$, respectively. Each of these matrices is of the form $A = [\boldsymbol{a_0}, \boldsymbol{a_1}, ... ,\boldsymbol{a_n}]$, where the vectors $\boldsymbol{a_k}$ represent the solutions at the discretized times $k \in 0,1,....,n$, where $n$ is the number of time points. In particular, the first column of $P$, denoted by $\boldsymbol{\rho_0}$, corresponds to the initial condition $\rho(\vec{x},0)$. If the spatial domain is one-dimensional, $P$, $Q$ and $W$ are of size $N \times n$, where $N$ is the number of spatial points. In the two-dimensional case, $P$ and $Q$ are of size $(N_1N_2) \times n$, where $N_1$ is the number of spatial points in the direction of $x_1$ and $N_2$ the points along the $x_2$ axis. Generally, $N_1 = N_2$. The discretized control $W$ for linear control problems is also $(N_1N_2) \times n$ dimensional, while it is $(2N_1N_2) \times n$ dimensional for nonlinear control problems. This is due to the fact that the control is represented by a vector field, when applied nonlinearly.
\\
\\
The optimization algorithm is initialized with a guess for the control, $W^{(0)}$. Then, in each iteration, denoted by $i$, the following steps are computed:
\vspace{0.1cm}
\begin{enumerate}
	\item Starting with a guess for the control $W^{(i)}$ as input variable, the corresponding state $P^{(i)}$ is found by solving the forward equation.
	\item In a next step, the value of the adjoint, $Q^{(i)}$, is established by computing the adjoint equation, using $W^{(i)}$ and $P^{(i)}$ as inputs. Since $P^{(i)}$ contains the solution for all discretized times $k \in 0,1,...,n$, this circumvents issues resulting from the non-local coupling in time, mentioned in Section \ref{sec:Method_PseudospectralPDECO}. As illustrated in the same section, time is reversed in the adjoint equation, so that the result is a matrix $\tilde{Q}^{(i)} =  [\boldsymbol{\adj_n},\boldsymbol{\adj_{n-1}}, ..., \boldsymbol{\adj_1} ]$. The columns of $\tilde{Q}^{(i)}$ are permuted to obtain the solution  $Q^{(i)}$.
	\item The gradient equation is solved, given the solutions $P^{(i)}$ and $Q^{(i)}$. This results in a new value for the control, $W^{(i)}_g$.
	\item  The convergence of the optimization scheme is measured by computing the error between $W^{(i)}$ and $W^{(i)}_{g}$. The error measure, $\mathcal{E}$, is discussed in detail in Section \ref{sec:Method_Validation}. 
	\begin{itemize}
		\item  If this error is lower than a set tolerance, the optimality system is self-consistent. This implies that the solution triplet ($\bar{P},\bar{W},\bar{Q}$) solves the (discretized) optimality system, and is therefore an optimal solution to the PDE-constrained optimization problem of interest.
		\item If the error is above the optimality tolerance, step 5 is executed.
	\end{itemize}
	\item Finally, the update $W^{(i+1)}$ is a linear combination of the current guess $W^{(i)}$, and the value obtained in step 3, $W^{(i)}_{g}$, employing a mixing rate $\lambda \in [0,1]$:
	\begin{align*}
	W^{(i+1)} = (1-\lambda)W^{(i)} + \lambda W^{(i)}_{g}.
	\end{align*}
	The guess for the control is updated from $W^{(i)} $ to $W^{(i+1)} $ and steps 1-5 are repeated until the method converges. 
\end{enumerate}
\vspace{0.3cm}
The update scheme in step 5, with mixing rate $\lambda$, is known to stabilise fixed point methods, ++Ben to add references++. Typical values of $\lambda$, which provide stable convergence, lie between $0.1$ and $0.001$. Throughout this paper, $\lambda =0.01$, unless stated otherwise. This mixing scheme is equivalent to the updating scheme presented in~\cite{Burger1}. 
Note that, while the solutions $P^{(i)}$ and $Q^{(i)}$ change in each iteration, the initial condition $\boldsymbol{\rho_0}$ and final time condition $\boldsymbol{\adj_n}$ remain unchanged throughout the process. Therefore, the only variable inducing a change in the solution is $W^{(i)}$.

++ Add something about errors -- mention to see below? ++

- showing why running ODE solver piecewise is more accurate than on the whole time line\\
- Chebyshev time points vs equispaced time grid for spectral accuracy\\
\\

- fixed point method: explain where it came from, adding mixing rate and why and that burger and ours are the same (derivation?)\\
- explain general functionality of the algorithm, the different input options, how the optimization algorithm works, what can be changed (e.g. norms, solvers, etc), what up to date is the fastest method, what is the most robust, etc. (here maybe the PDECO input PDF is helpful?)


\subsection{Inbuilt Matlab functions}
\subsubsection{The ODE solver}
- discuss different ODE solvers and which one is best for this and why
\subsubsection{The inbuilt optimization solver}
- here maybe some of the 'fsolveResearch' stuff?\\
- discuss how fsolve works (the review done on it etc) and what alternative is there in the literature\\

\section{Investigating Functionality of the Optimization Algorithms}
- Multiple shooting\\
- Fixed Point\\
- find a way to separate but join the following subsections for the two solvers\\
\subsection{Error measures}
- L2Linfinity Relative\\
- Pointwise Relative\\
- Absolute L1\\
- other?\\
- argue why relative is better, why one error measure is better than others etc
\subsubsection{L2Linfinity Relative Error}
++ Copied from paper, needs context ++
All errors in Section \ref{sec:Method_Validation} and Section \ref{sec:Expts} are calculated between a variable of interest, $y$, and $y_R$, the reference value that $y$ is compared to. When measuring convergence of the fixed point scheme, described in Section \ref{sec:Method_Solver}, $y = W^{(i)}_g$ and $y_R = W^{(i)}_i$. Alternatively, when investigating a known test problem, as done in Appendix \ref{app:TestProblems}, $y$ is a numerical solution and $y_R$ is an exact solution. The error measure $\mathcal{E}$ is composed of an $L^2$ error in space and an $L^\infty$ error in time. The relative $L^2$ error in the spatial direction is:
\begin{align*}
\mathcal{E}_{Rel}(t) = \frac{|| y(x,t) - y_{R}(x,t)||_{L^2(\Omega)} }{||y_R(x,t) + 10^{-10}||_{L^2(\Omega)}},
\end{align*}
where the small additional term on the denominator prevents division by zero.
Furthermore, the absolute $L^2$ error is:
\begin{align*}
\mathcal{E}_{Abs}(t) = || y(x,t) - y_R(x,t)||_{L^2(\Omega)}.
\end{align*}
Then, an $L^\infty$ error in time is taken of the minimum of $\mathcal{E}_{Rel}$ and $\mathcal{E}_{Abs}$, to obtain the error of interest:
\begin{align*}
\mathcal{E} = \max_{t \in [0,T]}\left[\min\left(\mathcal{E}_{Rel}(t), \mathcal{E}_{Abs}(t)\right)\right].
\end{align*}
The minimum between absolute and relative spatial error is taken to avoid choosing an erogenously large relative error, caused by division of one small term by another.

As a benchmark, we compared the fixed point scheme to Matlab's inbuilt \texttt{fsolve} function. It uses the trust-region-dogleg algorithm, see~\cite{Powell1}, to solve the optimality system of interest. While it is very robust, it is also much slower than the fixed point method, which works reliably for the types of problems considered in this paper. A comparison is given in Appendix \ref{app:fsolveComparison}. Numerical results for specific test problems with exact solutions are supplied in Appendix \ref{app:TestProblems}. Further tests to validate the method are presented in Appendix \ref{app:TestProblemsPerturbed}.


\subsection{Exact Solutions}
- all four problems \\
- mention that $w \sim \frac{1}{\beta}$ doesn't work well - should be on $p$ or both $\rho$ and $p$ instead \\
- compare different choices of exact solutions, i.e. linear, polynomial, exponential time; polynomial and trigonometric space \\
- show the performance of the polynomial vs. exponential for interpolation (this may have to go to 'tests on other parts of the code') \\
\subsection{Validation against different solvers}
- compare results of the same problem for fsolve, picard and fixed point method\\
- compare the different solvers. Compare how they measure convergence and which may do better or how they differ in general.\\
\\
\\
\input{Comparisonwithfsolve.tex}

\subsection{Perturbing $\hat \rho$}
- taking $\rho$ IC/IG of exact solution with $\beta_1$ and all other choices exact for $\beta_2$ and see what happens. \\
- other tests that change $\hat \rho$ instead of $w$. Can't remember exactly
\subsection{Perturbing $w$}
- discuss why the perturbation has to be smooth (in general and wrt the initial condition, give examples, error plots) AND check if that's still true given the knowledge on advection dominance and size of the problem\\
- perturbing in time \\
- perturbing in space \\
- perturbing in time and space \\
- symmetric/ asymmetric, different strengths\\
- relationship between this and the advection dominance\\
- show interpolation error in perturbed $w$\\
- perturb $\rho$ and show interpolation error there too\\
\\
\\
\input{Perturbingw.tex}
\subsection{Investigating changing $n$ and $N$}
- for both FW and optimization problem investigate how the error changes with the number of points\\
- investigate effect of beta and of tolerance settings\\

\subsection{Investigating changing tolerances}
- for both FW and optimization problem investigate how error changes with ODE tolerances (and include how it changes with not using the same tolerances for both RelTol and AbsTol)\\
- investigate interplay between this and number of points and beta
- for optimization problem do this for both ODE tolerances and optimality tolerances (whatever they are - do for different solvers)\\
- discuss how to choose tolerances given things such as interpolation errors and such.


\subsection{Investigating $\lambda$}
- what range of values is suitable\\
- interplay with $\beta$\\
- adaptive for different problems\\
- include here a study on how the convergence works: i.e. is it 'linear' over iterations, or is it behaving in any certain way. use this to justify adaptive approach
\subsection{Choice of $\hat \rho$}
- stationary vs moving\\
- achievable or not\\
- satisfying BCs and IC
\subsection{Choice of Initial Guess for $\adj$ (Multiple Shooting)}
- explain what has to be satisfied\\
- one idea: integrate from the gradient eqn. for initial guess of w. Check that this is still tricky with our new understanding of the exact solutions (too large, advection dominance, etc). \\
- other idea: one Kalise step to come up with p.\\
- show how much or how little the choice of initial guess for $p$ matters in toy examples.\\
- for both IGs show whether different IGs converge to the global minimum/ exact solution or whether it converges to some local minimum
\subsection{The relationship between diffusion and advection}
- investigate the size of the two terms \\
- when do different problems break because of advection dominance\\
- how does the interaction term come into this\\
- break something that works and fix something that doesn't, show breaking point and whether the relationship is abrupt or linear or what else\\
-check time interpolation for fixed x, does it get worse with larger solutions?
- Show example where it breaks eg 2D example with too steep desired state (advection dominance/ too steep gradients)\\
- investigate going from coarse grid to fine grid to push accuracy of solutions. see if it's to do with advection or ODE solver limitations.

\subsection{Tests on other parts of the code}
- showing why running ODE solver piecewise is more accurate than on the whole time line\\
- Chebyshev time points vs equispaced time grid for spectral accuracy (demo somewhere)
\subsubsection{Investigating interpolation errors}
- investigate the effect of interpolation error in interplay with tolerances, and number of points, and other factors.\\
- can be in link with perturbing $w$ too maybe?

\section{Examples}
- Neumann Flow\\
- Mass conservation, $\rho$ size $1$ for probability distribution\\
- symmetric\ asymmetric\\
- with interaction term \\
- 1D/2D \\
\\
- Dirichlet Flow \\
- Dirichlet Flow with $\rho$ size $1$ -- non-zero BCs ($0.5$ instead/ $0.25$ in 2D)
- Force Control (Dirichlet/Neumann) \\
- show how $w$ from zero in FW problem acts to achieve $\hat \rho$ working against or with interaction\\
- does the control focus on where the mass of the particles is?\\
- choose the examples in a way that each of them is making a point\\
- two peaks example and example with gaussian asymmetric $\hat \rho$\\
- plot space and time as surface plot (with colours) to show how the solution changes with time in 1D\\

++ See paper draft, section 5 (version from before Ben and John change things) for some inspiration on this. ++ \\
\\
\\
\input{NumericalExperiments.tex}

\section{Conclusion}



\pagebreak	
\bibliography{GeneralBib}
\bibliographystyle{unsrt}

\pagebreak
\appendix

\section{Other thoughts - things to incorporate above}
- mass correction if mass is to be one\\


\section{Useful Resources to go back to}
'A practical guide to pseudospectral methods', Bengt Fornberg\\
van kampen stochastic processes in physics and chemistry
\end{document}