\documentclass[11pt, a4paper]{article}
%\usepackage{proj1}
\usepackage{natbib}
\usepackage{fancyhdr}  
\usepackage{subcaption}
\usepackage{caption}
\usepackage{graphicx}
\linespread{1.25} 
\setlength{\parindent}{0cm}
\graphicspath{{Images/}}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{bbm}

%\usepackage[sc,osf]{mathpazo}
\usepackage{subcaption}
\usepackage[a4paper, top=1in, left=1.0in, right=1.0in, bottom=1in, includehead, includefoot]{geometry} %Usually have top as 1in

\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}


\hypersetup{colorlinks,linkcolor={black},citecolor={blue},urlcolor={black}}
\usepackage{color}
\urlstyle{same}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

%\newcommand{\Sta}{\rho}
\newcommand{\Adj}{p}
\newcommand{\adj}{q}
%\newcommand{\Con}{u}
\newcommand{\Sta}{\rho}
\newcommand{\Stav}{\mathbf{v}}
\newcommand{\Adja}{\mathbf{p}}
\newcommand{\Adjb}{q}
\newcommand{\Adjc}{{q}_{\partial \Sigma}}
\newcommand{\Con}{\mathbf{f}}
\newcommand{\nor}{\mathbf{n}}



\title{End Of Year Report/ Part of Thesis Draft}
\author{Jonna C. Roden\\ \\Supervision by Dr Ben Goddard and Dr John Pearson\\ \\ \vspace{0.5cm} MIGSAA}
\date{\today}


\pagenumbering{gobble}
\begin{document}
	\lstset{language=Matlab,%
		%basicstyle=\color{red},
		breaklines=true,%
		morekeywords={matlab2tikz},
		keywordstyle=\color{blue},%
		morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
		identifierstyle=\color{black},%
		stringstyle=\color{mylilas},
		commentstyle=\color{mygreen},%
		showstringspaces=false,%without this there will be a symbol in the places where there is a space
		numbers=left,%
		numberstyle={\tiny \color{black}},% size of the numbers
		numbersep=9pt, % this defines how far the numbers are from the text
		emph=[1]{for,end,break},emphstyle=[1]\color{blue}, %some words to emphasise
		%emph=[2]{word1,word2}, emphstyle=[2]{style},    
		basicstyle=\footnotesize\ttfamily,
	}
	
	\maketitle
	\begin{abstract}
		Sum up the below here!
		
	\end{abstract}
	
	\newpage
	\section*{Acknowledgements}
	+Acknowledge People Here+
	\newpage
	\pagenumbering{Roman} 
	\tableofcontents
	%\newpage
	%\listoffigures
	%\listoftables
	\newpage
	\pagenumbering{arabic} % Switch to normal numbers
	%\pagestyle{fancy}
















\section{Introduction}
++ Archer paper has examples for applications - also check extended project and other papers for this ++\\
+++get quotes right, figure out how ;) ++

\section{Literature Review on PDE-Constrained Optimization for these PDEs}
\input{MeanFieldOptimalControl.tex}
\section{Discussion of Relevant Equations and their Optimality Conditions}
\subsection{The Studied Equations}
This section is concerned with discussing different equations that have been studied in the past year and the derivation of the optimality conditions of optimal control problems with constrains involving these equations. The discussion of the forward equations and their connections is heavily based on \cite{Archer1}.

The most general equations considered, describing particle dynamics on a continuum level, are the so-called inertial equations. These are derived in \cite{Archer1}, by A.J.Archer, from the corresponding microscopic dynamics. The derivation includes taking momentum moments and making two modelling assumptions. The first assumption is that the contributions of particle interactions in the dynamic equations can be approximated by the interactions in equilibrium. The second assumption is a 'local-equilibrium' assumption, assuming that locally the velocity is normally distributed. This assumption is violated when steep velocity gradients arise, which will be discussed below.

The inertial equations are most generally formulated as:
\begin{align*}
	&\frac{\partial \Stav}{\partial t} + \Stav \cdot \nabla \Stav + \gamma \Stav = - \frac{1}{m} \nabla \frac{\delta \mathcal{F}[\Sta]}{\delta \Sta}\\
	&\frac{\partial \Sta}{\partial t} + \nabla \cdot (\Sta \Stav) =0
\end{align*}
This system of equations describes the evolution of a velocity field $\Stav$ and of the one-body particle density $\Sta$, which depends on the velocity field.
The velocity in the system is influenced by inertial effects, with friction coefficient $\gamma$, and by different forces, expressed in terms of the free energy $\mathcal{F}$ of the system.
In the following we choose $\mathcal{F}$ to be defined as:
\begin{align*}
\mathcal{F}[\Sta]=\int_\Omega  \bigg( V_{ext}\Sta + \Sta (\log \Sta -1) +  \frac{1}{2}\int_\Omega \Sta(r) \Sta(r')V_2(|r-r'|)dr' \bigg) dr.
\end{align*}
Taking the appropriate derivatives gives:
\begin{align*}
 \nabla \frac{\delta \mathcal{F}[\Sta]}{\delta \Sta} = \nabla V_{ext} + \nabla \ln \Sta + \int_\Omega \Sta(r') \nabla V_2(|r-r'|)dr',
\end{align*}
where $\nabla V_2$ is the force describing the particle interactions. However, in the derivation of corresponding optimality conditions, we instead consider a general interaction kernel $\mathbf{K}(r,r')$.
\\
Further to this general model, we introduce three more terms for modelling purposes. Two vector fields, $\mathbf{w}$ and $\Con$, are included in the velocity equation, which act as background flow fields in the problem. If these are conservative, they can be incorporated in the definition of $\nabla V_{ext}$. The term $\mathbf{w}$ will act as the flow control in the optimal control problem.
The final term that is added is a smoothing term for the velocity. This is to avoid steep velocity gradients, which are numerically challenging and violate the modelling assumptions outlined in \cite{Archer1}. Since steep velocity gradients are more prevalent in inertial systems, which have a small friction coefficient $\gamma$, the introduction of this additional term is standard practice, see \cite{Archer1} (+1 more?).
Including these terms leads to the model equations considered in this report:
\begin{align}
\label{eqn:INeqns1}
\frac{\partial \Stav}{\partial t} &+ (\Stav \cdot \nabla)\Stav + \gamma \Stav=\eta \nabla^2 \Stav  -\frac{1}{m}\Con +\frac{1}{m}\mathbf{w} - \frac{1}{m}\nabla \frac{\delta \mathcal{F}[\Sta]}{\delta \Sta}\\
\frac{\partial \Sta}{\partial t} &+ \nabla \cdot (\Sta \Stav)=0 \notag
\end{align}
The high friction limit of the inertial equations can be taken to derive the so-called overdamped eqation, see \cite{Archer1}. This is a numerically easier problem, which only involves the variable $\rho$, and not $\Stav$ and is therefore a good starting point when developing a new numerical algorithm for their optimal control. 
The overdamped eqaution is derived by assuming that for large $\gamma$ the material derivative of $\Sta$, $\frac{D \Sta}{D t} \coloneqq  \frac{\partial \Stav}{\partial t} + (\Stav \cdot \nabla)\Stav$ is zero.
Then Equations (\ref{eqn:INeqns1}) reduce to:
 \begin{align*}
&\gamma \Stav=\eta \nabla^2 \Stav  -\frac{1}{m}\Con +\frac{1}{m}\mathbf{w} - \frac{1}{m}\nabla \frac{\delta \mathcal{F}[\Sta]}{\delta \Sta}\\
 &\frac{\partial \Sta}{\partial t} + \nabla \cdot (\Sta \Stav)=0 \notag.
 \end{align*}
Then, $\Stav$ can be substituted in the evolution equation for $\rho$, and the smoothing term for $\Stav$ can be neglected, since the high friction limit is taken and the reason for its introduction hence vanishes. The overdamped equation is:
  \begin{align*}
 &\frac{\partial \Sta}{\partial t} -\frac{1}{m \gamma}\nabla \cdot (\Sta\Con) +\frac{1}{m \gamma} \nabla \cdot (\Sta \mathbf{w}) - \frac{1}{m \gamma}\nabla \cdot \bigg(\Sta\nabla \frac{\delta \mathcal{F}[\Sta]}{\delta \Sta}\bigg)=0 \notag.
 \end{align*}
 In particular, substituting the choice of free energy introduced above, and using that $\nabla \rho = \rho\nabla \ln \rho$, we get:
\begin{align*}
\frac{\partial \Sta}{\partial t} &= \frac{1}{m \gamma}\nabla \cdot (\Sta\Con) -\frac{1}{m \gamma} \nabla \cdot (\Sta \mathbf{w})  + \frac{1}{m \gamma}\nabla \cdot (\rho\nabla V_{ext}) + \frac{1}{m \gamma}\nabla \cdot (\nabla \rho) \\
&+\frac{1}{m \gamma}\nabla \cdot \int_\Omega \Sta(r)\Sta(r') \nabla V_2(|r-r'|)dr'\notag.
\end{align*}
The overdamped equation that is considered in this report, is found by rescaling time: $t = \tilde t \gamma m$. This causes the constants to cancel, and implies that comparison between (\ref{eqn:INeqns1}) and (\ref{eqn:ADeqn1}) need to be made on the two different time scales.
The resulting equation is:
\begin{align*}
\label{eqn:ADeqn1}
\frac{\partial \Sta}{\partial \tilde t} &= \nabla \cdot (\Sta\Con) - \nabla \cdot (\Sta \mathbf{w})  + \nabla \cdot (\rho\nabla V_{ext}) + \nabla \cdot (\nabla \rho) \\
&+\nabla \cdot \int_\Omega \Sta(r)\Sta(r') \nabla V_2(|r-r'|)dr'.
\end{align*}
\subsection{Optimality Conditions for the Inertial Eqations}


\input{OptimalityConditionsArcherNew.tex}

\subsection{Optimality Conditions for the Overdamped Equations}
The optimality conditions for the optimal control problems involving the overdamped equations \eqref{eqn:ADeqn1} are stated here for completion. The details of their derivation can be found in either the year one report or the paper (++ need to find how to cite++).
There are two optimal control problems considered; one which applies the control through the flow field, as above, and another, where the control is an added source term in the PDE. The latter case is less physical, however, it is often a simpler problem to study because the control is applied linearly, while the flow control problem considers a non-linear control. For each problem, no-flux and Dirichlet boundary conditions are considered. Note that, for ease of notation, we set $\tilde t = t$.
The flow control optimal control problem is:
\begin{align*}
&\min_{\Sta,\mathbf{w} } \mathcal J(\Sta,\mathbf{w} ) =\ \ \frac{1}{2}||\Sta - \hat \Sta||_{L_2(\Sigma)}^2  +\frac{\beta}{2}||\mathbf{w}||_{L_2(\Sigma)}^2\\
& \text{subject to:}:\\
&\frac{\partial \Sta}{\partial t} = \nabla \cdot (\Sta\Con) - \nabla \cdot (\Sta \mathbf{w})  + \nabla \cdot (\rho\nabla V_{ext}) + \nabla \cdot (\nabla \rho) \\
& \quad \ +\nabla \cdot \int_\Omega \Sta(r)\Sta(r') \nabla V_2(|r-r'|)dr' \qquad \qquad \qquad \qquad \text{in} \quad \Sigma
\end{align*}
The adjoint and gradient equations are:
\begin{align*}
\frac{\partial \Adjb}{\partial t} =& - \nabla^2\Adjb - \mathbf{w} \cdot \nabla \Adjb + \nabla V_{ext} \cdot \nabla \Adjb - \Sta + \hat \rho \\
&+\kappa \int_\Omega (\nabla_r \Adjb(r) - \nabla_{r'} \Adjb(r') ) \rho(r') \mathbf{K}(r,r') dr'\\
\mathbf{w} =& - \frac{1}{\beta} \Sta \nabla \Adjb,
\end{align*}
where $\Adjb$ is the adjoint variable and $\frac{\partial \Adjb}{\partial n} = 0$ on $\partial \Omega$ corresponds to a no-flux boundary condition, while $\Adjb = 0$ on $\partial \Omega$ corresponds to a Dirichlet boundary condition.
Rewriting the time variable in the adjoint equation as $\tau = T-t$ gives:
\begin{align*}
\frac{\partial \Adjb}{\partial \tau} =& \nabla^2\Adjb +\mathbf{w} \cdot \nabla \Adjb - \nabla V_{ext} \cdot \nabla \Adjb + \Sta - \hat \rho \\
&-\kappa \int_\Omega (\nabla_r \Adjb(r) - \nabla_{r'} \Adjb(r') ) \rho(r') \mathbf{K}(r,r') dr'\\
\mathbf{w} =& - \frac{1}{\beta} \Sta \nabla \Adjb,
\end{align*}

The source control optimal control problem is:
\begin{align*}
&\min_{\Sta,{w} } \mathcal J(\Sta,{w}) = \ \ \frac{1}{2}||\Sta - \hat \Sta||_{L_2(\Sigma)}^2  +\frac{\beta}{2}||{w}||_{L_2(\Sigma)}^2\\
& \text{subject to:}:\\
&\frac{\partial \Sta}{\partial  t} = \nabla \cdot (\Sta\Con) + \nabla \cdot (\rho\nabla V_{ext}) + \nabla \cdot (\nabla \rho) + w \\
& \quad \ +\nabla \cdot \int_\Omega \Sta(r)\Sta(r') \nabla V_2(|r-r'|)dr' \qquad \qquad \qquad \qquad \text{in} \quad \Sigma
\end{align*}

The adjoint and gradient equations are:
\begin{align*}
\frac{\partial \Adjb}{\partial t} =& - \nabla^2 \Adjb + \nabla V_{ext} \cdot \nabla \Adjb - \Sta + \hat \rho \\
&+\kappa \int_\Omega (\nabla_r \Adjb(r) - \nabla_{r'} \Adjb(r') ) \rho(r') \mathbf{K}(r,r') dr'\\
\mathbf{w} =& - \frac{1}{\beta} \Adjb,
\end{align*}
Boundary conditions and time reversal for the adjoint equation are analogous to the flow control problem.

\subsection{Subdomain and Boundary Observation with Non-Constant Flux}
\input{NonconstantFluxSubdomainObservation.tex}
\input{NonconstantFluxBoundaryObservation.tex}
\section{Numerical Methods}
In general it is necessary to change the time variable in the adjoint equation, as demonstrated in (++ ++ ), for numerical stability. This is necessary because the forward and adjoint equations contain Laplacians of opposite time. Running the adjoint equation with a negative Laplacian leads to a blow up of the solution at the first time step. The reversal of time, using $\tau = T-t$, remedies this issue, however, this causes a non-local coupling in time between the two PDEs.
The following algorithms provide methods of treating this non-local coupling.
\subsection{Picard Multiple Shooting}
The multiple shooting algorithm, introduced in the first year report, has been extended by employing a Picard mixing scheme as optimization method to replace \texttt{fsolve}. In the following, this is briefly outlined.
The multiple shooting method consists of discretizing the time interval and solving the optimality system on each interval individually. This is done because of the non-local time coupling of the forward and adjoint equations. It requires the input of an initial guess at each time for each of the variables. The aim of the optimization solver is then to minimize the distance between the initial guesses and numerical solutions of the variables at each of the time points. \\
The Picard mixing scheme is a fixed point type algorithm. At each iteration $i$ it takes a set of guesses at the discretized time points, denoted by $Y_i$. The matrix $Y$ contains the discretized values for the variables $\rho$ and $q$, at each time and space point.
Then the system of PDEs on the discretized intervals is computed and a new set of variable values at the time points is created, denoted by $Y_{out}$. Then, the mixing scheme provides a new guess for the iteration $i+1$ by:
\begin{align*}
Y_{i+1} = (1 - \lambda)Y_i + \lambda Y_{out},
\end{align*}
where $\lambda$ is the mixing rate. It typically takes values between $0.1$ and $0.01$, depending on the complexity of the system to solve. Choosing a relatively small value of $\lambda$ stabilizes the algorithm (+ ref this?+). 
The algorithm terminates when the system of PDEs is solved self-consistently, i.e. when the distance between $Y_i$ and $Y_{out}$ is small, as measured in a chosen norm. Different norms are discussed in Section ++. (Will we do that?)
This algorithm is working very well for examples involving the overdamped equations. However, in the following an even simpler algorithm is presented, which does not require the solution of the optimality system on small time intervals and is therefore even quicker. However, since we will apply the numerical optimization method to increasingly difficult optimal control problems, the multiple shooting algorithm may provide more numerical stability for numerically harder problems and is therefore a relevant tool to consider in the future. Changing the optimization solver in the implementation is straightforward and requires only changing a flag in the input file.

\subsection{Fixed Point Algorithm}
- Kalise and Burger papers, sweeping algorithms (and our adaptation - or adaptation in next section)\\

In this section we describe the fixed point algorithm, which includes a similar idea to the multiple shooting code, by employing a mixing scheme to update the guess at each iteration. While the multiple shooting algorithm updates through the variables $\rho$ and $q$, the fixed point algorithm updates through the control variable. This algorithm is explained in more detail.
We denote the discretized versions of the variables $\rho$, $\adj$ and $\vec{w}$ with $P$, $Q$ and $W$, respectively. Each of these matrices is of the form $A = [\boldsymbol{a_0}, \boldsymbol{a_1}, ... ,\boldsymbol{a_n}]$, where the vectors $\boldsymbol{a_k}$ represent the solutions at the discretized times $k \in 0,1,....,n$, where $n$ is the number of time points. In particular, the first column of $P$, denoted by $\boldsymbol{\rho_0}$, corresponds to the initial condition $\rho(r,0)$. If the spatial domain is one-dimensional, $P$, $Q$ and $W$ are of size $N \times (n + 1)$, where $N$ is the number of spatial points. In the two-dimensional case, $P$ and $Q$ are of size $(N_1N_2) \times (n + 1)$, where $N_1$ is the number of spatial points in the direction of $x_1$ and $N_2$ the points along the $x_2$ axis. Generally, $N_1 = N_2$. The discretized control $W$ for linear control problems is also $(N_1N_2) \times (n + 1)$ dimensional, while it is $(2N_1N_2) \times (n + 1)$ dimensional for nonlinear control problems. This is due to the fact that the control is represented by a vector field, when applied nonlinearly.
\\
\\
The optimization algorithm is initialized with a guess for the control, $W^{(0)}$. Then, in each iteration, denoted by $i$, the following steps are computed:
\vspace{0.1cm}
\begin{enumerate}
	\item Starting with a guess for the control $W^{(i)}$ as input variable, the corresponding state $P^{(i)}$ is found by solving the forward equation.
	\item In a next step, the value of the adjoint, $Q^{(i)}$, is established by computing the adjoint equation, using $W^{(i)}$ and $P^{(i)}$ as inputs. Since $P^{(i)}$ contains the solution for all discretized times $k \in 0,1,...,n$, this circumvents issues resulting from the non-local coupling in time, resulting from reversing time in the adjoint equation. As illustrated in the same section, time is reversed in the adjoint equation, so that the result is a matrix $\tilde{Q}^{(i)} =  [\boldsymbol{\adj_n},\boldsymbol{\adj_{n-1}}, ..., \boldsymbol{\adj_1} ]$. The columns of $\tilde{Q}^{(i)}$ are permuted to obtain the solution  $Q^{(i)}$.
	\item The gradient equation is solved, given the solutions $P^{(i)}$ and $Q^{(i)}$. This results in a new value for the control, $W^{(i)}_g$.
	\item  The convergence of the optimization scheme is measured by computing the error between $W^{(i)}$ and $W^{(i)}_{g}$. The error measure, $\mathcal{E}$, is discussed in detail in Section \ref{sec:Method_Validation}. 
	\begin{itemize}
		\item  If this error is lower than a set tolerance, the optimality system is self-consistent. This implies that the solution triplet ($\bar{P},\bar{W},\bar{Q}$) solves the (discretized) optimality system, and is therefore an optimal solution to the PDE-constrained optimization problem of interest.
		\item If the error is above the optimality tolerance, step 5 is executed.
	\end{itemize}
	\item Finally, the update $W^{(i+1)}$ is a linear combination of the current guess $W^{(i)}$, and the value obtained in step 3, $W^{(i)}_{g}$, employing a mixing rate $\lambda \in [0,1]$:
	\begin{align*}
	W^{(i+1)} = (1-\lambda)W^{(i)} + \lambda W^{(i)}_{g}.
	\end{align*}
	The guess for the control is updated from $W^{(i)} $ to $W^{(i+1)} $ and steps 1-5 are repeated until the method converges. 
\end{enumerate}
\vspace{0.3cm}
The update scheme in step 5, with mixing rate $\lambda$, is known to stabilise fixed point methods, ++Ben to add references++. Typical values of $\lambda$, which provide stable convergence, lie between $0.1$ and $0.001$. Throughout this paper, $\lambda =0.01$, unless stated otherwise. This mixing scheme is equivalent to the updating scheme presented in~\cite{Burger1}. 
Note that, while the solutions $P^{(i)}$ and $Q^{(i)}$ change in each iteration, the initial condition $\boldsymbol{\rho_0}$ and final time condition $\boldsymbol{\adj_n}$ remain unchanged throughout the process. Therefore, the only variable inducing a change in the solution is $W^{(i)}$.

++ Add something about errors -- mention to see below? ++

- showing why running ODE solver piecewise is more accurate than on the whole time line\\
- Chebyshev time points vs equispaced time grid for spectral accuracy\\
\\

- fixed point method: explain where it came from, adding mixing rate and why and that burger and ours are the same (derivation?)\\
- explain general functionality of the algorithm, the different input options, how the optimization algorithm works, what can be changed (e.g. norms, solvers, etc), what up to date is the fastest method, what is the most robust, etc. (here maybe the PDECO input PDF is helpful?)


\subsection{Inbuilt Matlab functions}
\subsubsection{The ODE solver}
- discuss different ODE solvers and which one is best for this and why
\subsubsection{The inbuilt optimization solver}
- here maybe some of the 'fsolveResearch' stuff?\\
- discuss how fsolve works (the review done on it etc) and what alternative is there in the literature\\

\section{Investigating Functionality of the Optimization Algorithms}
- Multiple shooting\\
- Fixed Point\\
- find a way to separate but join the following subsections for the two solvers\\
\subsection{Error measures}
- L2Linfinity Relative\\
- Pointwise Relative\\
- Absolute L1\\
- other?\\
- argue why relative is better, why one error measure is better than others etc
\subsubsection{L2Linfinity Relative Error}
++ Copied from paper, needs context ++
All errors in Section \ref{sec:Method_Validation} and Section \ref{sec:Expts} are calculated between a variable of interest, $y$, and $y_R$, the reference value that $y$ is compared to. When measuring convergence of the fixed point scheme, described in Section \ref{sec:Method_Solver}, $y = W^{(i)}_g$ and $y_R = W^{(i)}_i$. Alternatively, when investigating a known test problem, as done in Appendix \ref{app:TestProblems}, $y$ is a numerical solution and $y_R$ is an exact solution. The error measure $\mathcal{E}$ is composed of an $L^2$ error in space and an $L^\infty$ error in time. The relative $L^2$ error in the spatial direction is:
\begin{align*}
\mathcal{E}_{Rel}(t) = \frac{|| y(x,t) - y_{R}(x,t)||_{L^2(\Omega)} }{||y_R(x,t) + 10^{-10}||_{L^2(\Omega)}},
\end{align*}
where the small additional term on the denominator prevents division by zero.
Furthermore, the absolute $L^2$ error is:
\begin{align*}
\mathcal{E}_{Abs}(t) = || y(x,t) - y_R(x,t)||_{L^2(\Omega)}.
\end{align*}
Then, an $L^\infty$ error in time is taken of the minimum of $\mathcal{E}_{Rel}$ and $\mathcal{E}_{Abs}$, to obtain the error of interest:
\begin{align*}
\mathcal{E} = \max_{t \in [0,T]}\left[\min\left(\mathcal{E}_{Rel}(t), \mathcal{E}_{Abs}(t)\right)\right].
\end{align*}
The minimum between absolute and relative spatial error is taken to avoid choosing an erogenously large relative error, caused by division of one small term by another.

As a benchmark, we compared the fixed point scheme to Matlab's inbuilt \texttt{fsolve} function. It uses the trust-region-dogleg algorithm, see~\cite{Powell1}, to solve the optimality system of interest. While it is very robust, it is also much slower than the fixed point method, which works reliably for the types of problems considered in this paper. A comparison is given in Appendix \ref{app:fsolveComparison}. Numerical results for specific test problems with exact solutions are supplied in Appendix \ref{app:TestProblems}. Further tests to validate the method are presented in Appendix \ref{app:TestProblemsPerturbed}.



\subsection{The relationship between diffusion and advection}
- investigate the size of the two terms \\
- when do different problems break because of advection dominance\\
- how does the interaction term come into this\\
- break something that works and fix something that doesn't, show breaking point and whether the relationship is abrupt or linear or what else\\
-check time interpolation for fixed x, does it get worse with larger solutions?
- Show example where it breaks eg 2D example with too steep desired state (advection dominance/ too steep gradients)\\
- investigate going from coarse grid to fine grid to push accuracy of solutions. see if it's to do with advection or ODE solver limitations.\\
\\
\\
Mention here:\\
Exact Solutions:\\
- mention that $w \sim \frac{1}{\beta}$ doesn't work well - should be on $p$ or both $\rho$ and $p$ instead \\
- compare different choices of exact solutions, i.e. linear, polynomial, exponential time; polynomial and trigonometric space \\
- show the performance of the polynomial vs. exponential for interpolation (this may have to go to 'tests on other parts of the code') \\
\subsection{Validation against \texttt{fsolve}}
- compare results of the same problem for fsolve, picard and fixed point method\\
- compare the different solvers. Compare how they measure convergence and which may do better or how they differ in general.\\
\\
\\
\input{Comparisonwithfsolve.tex}

\subsection{Perturbing $w$}
- discuss why the perturbation has to be smooth (in general and wrt the initial condition, give examples, error plots) AND check if that's still true given the knowledge on advection dominance and size of the problem\\
- perturbing in time \\
- perturbing in space \\
- perturbing in time and space \\
- symmetric/ asymmetric, different strengths\\
- relationship between this and the advection dominance\\
- show interpolation error in perturbed $w$\\
- perturb $\rho$ and show interpolation error there too\\
\\
\\
\input{Perturbingw.tex}
\subsection{Note on investigating changing $n$ and $N$, tolerances, $\lambda$}
- for both FW and optimization problem investigate how the error changes with the number of points\\
- investigate effect of beta and of tolerance settings\\

- for both FW and optimization problem investigate how error changes with ODE tolerances (and include how it changes with not using the same tolerances for both RelTol and AbsTol)\\
- investigate interplay between this and number of points and beta
- for optimization problem do this for both ODE tolerances and optimality tolerances (whatever they are - do for different solvers)\\
- discuss how to choose tolerances given things such as interpolation errors and such.\\
\\
\\
Notes on:\\
Choice of $\hat \rho$\\
- stationary vs moving\\
- achievable or not\\
- satisfying BCs and IC\\
Choice of Initial Guess for $\adj$ (Multiple Shooting):\\
- explain what has to be satisfied\\
- one idea: integrate from the gradient eqn. for initial guess of w. Check that this is still tricky with our new understanding of the exact solutions (too large, advection dominance, etc). \\
- other idea: one Kalise step to come up with p.\\
- show how much or how little the choice of initial guess for $p$ matters in toy examples.\\
- for both IGs show whether different IGs converge to the global minimum/ exact solution or whether it converges to some local minimum\\
\\
\\
Tests on other parts of the code:
- showing why running ODE solver piecewise is more accurate than on the whole time line\\
- Chebyshev time points vs equispaced time grid for spectral accuracy (demo somewhere)\\
\\
\\
Investigating interpolation errors\\
- investigate the effect of interpolation error in interplay with tolerances, and number of points, and other factors.\\
- can be in link with perturbing $w$ too maybe?

\section{Examples}
- Neumann Flow\\
- Mass conservation, $\rho$ size $1$ for probability distribution\\
- symmetric\ asymmetric\\
- with interaction term \\
- 1D/2D \\
\\
- Dirichlet Flow \\
- Dirichlet Flow with $\rho$ size $1$ -- non-zero BCs ($0.5$ instead/ $0.25$ in 2D)
- Force Control (Dirichlet/Neumann) \\
- show how $w$ from zero in FW problem acts to achieve $\hat \rho$ working against or with interaction\\
- does the control focus on where the mass of the particles is?\\
- choose the examples in a way that each of them is making a point\\
- two peaks example and example with gaussian asymmetric $\hat \rho$\\
- plot space and time as surface plot (with colours) to show how the solution changes with time in 1D\\

++ See paper draft, section 5 (version from before Ben and John change things) for some inspiration on this. ++ \\
\\
\\
+++ Include examples with changed $V^{ext}$ -- more interesting +++
\input{NumericalExperiments.tex}

\section{Conclusion}
+ outlook! (what are we doing next?)\\
+ additional activities of what I've been doing outside the project.


\pagebreak	
\bibliography{GeneralBib}
\bibliographystyle{unsrt}

\pagebreak
\appendix

\section{Other thoughts - things to incorporate above}
- mass correction if mass is to be one\\


\section{Useful Resources to go back to}
'A practical guide to pseudospectral methods', Bengt Fornberg\\
van kampen stochastic processes in physics and chemistry
\end{document}